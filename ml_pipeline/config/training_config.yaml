# ML Model Training Configuration

# Data
data:
  train_test_split: 0.2  # 80% train, 20% test
  validation_split: 0.2  # Of training data, 20% for validation
  random_state: 42

# Feature Engineering
features:
  # Cyclical encoding for time features
  encode_time_cyclical: true

  # Feature scaling
  scale_features: true
  scaler_type: 'standard'  # 'standard', 'minmax', 'robust'

  # Feature selection
  remove_low_variance: true
  variance_threshold: 0.01

# XGBoost Hyperparameters (From tuned_config.yaml - Optimized via hyperparameter tuning)
xgboost:
  profit_predictor:
    n_estimators: 1500  # Reduced from 4500 - early stopping was kicking in at ~1500 anyway
    max_depth: 12  # Reduced from 14 - shallower trees
    learning_rate: 0.015  # Increased from 0.01 - learn faster
    min_child_weight: 10
    subsample: 0.95  # Reduced from 0.998 - simpler trees
    colsample_bytree: 0.7390278059058913
    gamma: 0.18660209052539536
    reg_alpha: 0.10404086524316918
    reg_lambda: 4.6210183668159015
    objective: 'reg:squarederror'
    eval_metric: 'rmse'
    tree_method: 'hist'
    early_stopping_rounds: 100  # Reduced from 150

  success_classifier:
    n_estimators: 1500
    max_depth: 11
    learning_rate: 0.03129926620588006
    min_child_weight: 10
    subsample: 0.9890898915737059
    colsample_bytree: 0.7780874908732436
    gamma: 0.2828364305968697
    reg_alpha: 0.49326562706568244
    reg_lambda: 3.3253962521475
    scale_pos_weight: 4.79  # Updated for optimal hindsight data: 17.3% success rate (84898/17735 = 4.79)
    objective: 'binary:logistic'
    eval_metric: 'auc'
    tree_method: 'hist'
    early_stopping_rounds: 150

  duration_predictor:
    n_estimators: 1000  # Reduced from 3000 - fewer trees
    max_depth: 12  # Reduced from 14 - shallower trees
    learning_rate: 0.015  # Increased from 0.0067 - learn faster, need fewer trees
    min_child_weight: 10
    subsample: 0.95  # Reduced from 0.9997 - less data per tree = simpler trees
    colsample_bytree: 0.7344606289549767
    gamma: 0.053723598184720445
    reg_alpha: 0.046829834272052326
    reg_lambda: 4.649171098015957
    objective: 'reg:squarederror'
    eval_metric: 'rmse'
    tree_method: 'hist'
    early_stopping_rounds: 100  # Reduced from 150 - stop earlier if not improving

# LightGBM Hyperparameters
lightgbm:
  profit_predictor:
    n_estimators: 500
    max_depth: 6
    learning_rate: 0.05
    num_leaves: 31
    min_child_samples: 20
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 1.0
    objective: 'regression'
    metric: 'rmse'
    early_stopping_rounds: 50

  success_classifier:
    n_estimators: 300
    max_depth: 5
    learning_rate: 0.05
    num_leaves: 31
    min_child_samples: 20
    subsample: 0.8
    colsample_bytree: 0.8
    objective: 'binary'
    metric: 'auc'
    early_stopping_rounds: 50

  duration_predictor:
    n_estimators: 400
    max_depth: 6
    learning_rate: 0.05
    num_leaves: 31
    min_child_samples: 20
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 1.0
    objective: 'regression'
    metric: 'rmse'
    early_stopping_rounds: 50

# CatBoost Hyperparameters
catboost:
  profit_predictor:
    iterations: 500
    depth: 6
    learning_rate: 0.05
    l2_leaf_reg: 1.0
    loss_function: 'RMSE'
    early_stopping_rounds: 50

  success_classifier:
    iterations: 300
    depth: 5
    learning_rate: 0.05
    l2_leaf_reg: 1.0
    loss_function: 'Logloss'
    early_stopping_rounds: 50

  duration_predictor:
    iterations: 400
    depth: 6
    learning_rate: 0.05
    l2_leaf_reg: 1.0
    loss_function: 'RMSE'
    early_stopping_rounds: 50

# Random Forest Hyperparameters
random_forest:
  profit_predictor:
    n_estimators: 200
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: 'sqrt'

  success_classifier:
    n_estimators: 200
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: 'sqrt'
    class_weight: 'balanced'

  duration_predictor:
    n_estimators: 200
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: 'sqrt'

# Training
training:
  verbose: true
  save_models: true
  save_feature_importance: true

# ONNX Export
onnx:
  target_opset: 12
  export_format: 'onnx'
