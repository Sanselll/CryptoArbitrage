================================================================================
TRAINING PPO AGENT FOR FUNDING ARBITRAGE
================================================================================
Data: data/rl_train.csv
Total timesteps: 200,000
Learning rate: 0.0003
Seed: 42

Creating training environment with 8 parallel workers...
Creating evaluation environment...
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

Initializing PPO agent...
💻 Using CPU (faster than MPS for MLP + small batches)
Using cpu device

Model architecture:
  Policy network: MLP (256 → 256 hidden layers)
  Value network: MLP (256 → 256 hidden layers)
  Total parameters: ~200K per network
  Observation space: 124 dimensions
  Action space: 9 discrete actions
  Entropy: 0.0800 → 0.0200 (annealed via callback)
  Gamma: 0.9900
  GAE Lambda: 0.9800
  Clip range: 0.243
  Reward scale: 1.000
  Hold bonus: 0.000
  Quality entry bonus: 0.500
  Quality entry penalty: -0.500

================================================================================
STARTING TRAINING
================================================================================
Training for 200,000 timesteps...
Progress will be logged to: models/rl_v3/ppo_20251031_114621

Logging to models/rl_v3/ppo_20251031_114621/tensorboard/PPO_1
/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x32e2438b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x32e2437f0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -20.7    |
| time/                    |          |
|    fps                   | 354      |
|    iterations            | 1        |
|    time_elapsed          | 46       |
|    total_timesteps       | 16384    |
| trading/                 |          |
|    episode_pnl_pct       | -0.356   |
|    episode_reward        | -27      |
|    final_portfolio_value | 9.96e+03 |
|    reward_per_pnl        | 75.8     |
|    trades_count          | 22       |
|    win_rate              | 18.2     |
| train/                   |          |
|    ent_coef              | 0.0751   |
---------------------------------------
-------------------------------------------
| rollout/                 |              |
|    ep_len_mean           | 72           |
|    ep_rew_mean           | -30.9        |
| time/                    |              |
|    fps                   | 345          |
|    iterations            | 2            |
|    time_elapsed          | 94           |
|    total_timesteps       | 32768        |
| trading/                 |              |
|    episode_pnl_pct       | -0.4         |
|    episode_reward        | -30          |
|    final_portfolio_value | 9.96e+03     |
|    reward_per_pnl        | 74.9         |
|    trades_count          | 22           |
|    win_rate              | 13.6         |
| train/                   |              |
|    approx_kl             | 0.0141827315 |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.243        |
|    ent_coef              | 0.0702       |
|    entropy_loss          | -2.19        |
|    explained_variance    | -0.00142     |
|    learning_rate         | 0.0003       |
|    loss                  | 226          |
|    n_updates             | 17           |
|    policy_gradient_loss  | -0.0229      |
|    value_loss            | 715          |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-71.59 +/- 119.21
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | -71.6       |
| time/                    |             |
|    total_timesteps       | 40000       |
| trading/                 |             |
|    episode_pnl_pct       | -0.946      |
|    episode_reward        | -114        |
|    final_portfolio_value | 9.91e+03    |
|    reward_per_pnl        | 121         |
|    trades_count          | 21          |
|    win_rate              | 23.8        |
| train/                   |             |
|    approx_kl             | 0.011715308 |
|    clip_fraction         | 0.0542      |
|    clip_range            | 0.243       |
|    entropy_loss          | -2.18       |
|    explained_variance    | -0.0146     |
|    learning_rate         | 0.0003      |
|    loss                  | 861         |
|    n_updates             | 34          |
|    policy_gradient_loss  | -0.0249     |
|    value_loss            | 963         |
------------------------------------------
New best mean reward!
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -24      |
| time/                    |          |
|    fps                   | 351      |
|    iterations            | 3        |
|    time_elapsed          | 139      |
|    total_timesteps       | 49152    |
| trading/                 |          |
|    episode_pnl_pct       | -0.349   |
|    episode_reward        | -29.5    |
|    final_portfolio_value | 9.97e+03 |
|    reward_per_pnl        | 84.4     |
|    trades_count          | 21       |
|    win_rate              | 14.3     |
| train/                   |          |
|    ent_coef              | 0.0653   |
---------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -16.2       |
| time/                    |             |
|    fps                   | 373         |
|    iterations            | 4           |
|    time_elapsed          | 175         |
|    total_timesteps       | 65536       |
| trading/                 |             |
|    episode_pnl_pct       | -0.269      |
|    episode_reward        | -32.4       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 121         |
|    trades_count          | 19          |
|    win_rate              | 15.8        |
| train/                   |             |
|    approx_kl             | 0.013747171 |
|    clip_fraction         | 0.0713      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0603      |
|    entropy_loss          | -2.17       |
|    explained_variance    | -0.0291     |
|    learning_rate         | 0.0003      |
|    loss                  | 327         |
|    n_updates             | 51          |
|    policy_gradient_loss  | -0.0282     |
|    value_loss            | 858         |
------------------------------------------
Eval num_timesteps=80000, episode_reward=147.92 +/- 114.10
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | 148         |
| time/                    |             |
|    total_timesteps       | 80000       |
| trading/                 |             |
|    episode_pnl_pct       | -0.285      |
|    episode_reward        | -26.8       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 94.1        |
|    trades_count          | 19          |
|    win_rate              | 21.1        |
| train/                   |             |
|    approx_kl             | 0.012124584 |
|    clip_fraction         | 0.0668      |
|    clip_range            | 0.243       |
|    entropy_loss          | -2.15       |
|    explained_variance    | 0.0116      |
|    learning_rate         | 0.0003      |
|    loss                  | 382         |
|    n_updates             | 68          |
|    policy_gradient_loss  | -0.0234     |
|    value_loss            | 2.17e+03    |
------------------------------------------
New best mean reward!
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -25.8    |
| time/                    |          |
|    fps                   | 369      |
|    iterations            | 5        |
|    time_elapsed          | 221      |
|    total_timesteps       | 81920    |
| trading/                 |          |
|    episode_pnl_pct       | 0.00962  |
|    episode_reward        | 6.97     |
|    final_portfolio_value | 1e+04    |
|    reward_per_pnl        | 724      |
|    trades_count          | 21       |
|    win_rate              | 28.6     |
| train/                   |          |
|    ent_coef              | 0.0554   |
---------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -21.7       |
| time/                    |             |
|    fps                   | 361         |
|    iterations            | 6           |
|    time_elapsed          | 272         |
|    total_timesteps       | 98304       |
| trading/                 |             |
|    episode_pnl_pct       | -0.332      |
|    episode_reward        | -34.2       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 103         |
|    trades_count          | 19          |
|    win_rate              | 5.26        |
| train/                   |             |
|    approx_kl             | 0.016697142 |
|    clip_fraction         | 0.0958      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0505      |
|    entropy_loss          | -2.14       |
|    explained_variance    | -0.0515     |
|    learning_rate         | 0.0003      |
|    loss                  | 179         |
|    n_updates             | 85          |
|    policy_gradient_loss  | -0.0351     |
|    value_loss            | 727         |
------------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -15.8       |
| time/                    |             |
|    fps                   | 369         |
|    iterations            | 7           |
|    time_elapsed          | 310         |
|    total_timesteps       | 114688      |
| trading/                 |             |
|    episode_pnl_pct       | -0.261      |
|    episode_reward        | -23.2       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 88.9        |
|    trades_count          | 17          |
|    win_rate              | 41.2        |
| train/                   |             |
|    approx_kl             | 0.014601146 |
|    clip_fraction         | 0.085       |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0456      |
|    entropy_loss          | -2.13       |
|    explained_variance    | 0.023       |
|    learning_rate         | 0.0003      |
|    loss                  | 499         |
|    n_updates             | 102         |
|    policy_gradient_loss  | -0.0319     |
|    value_loss            | 840         |
------------------------------------------
Eval num_timesteps=120000, episode_reward=-78.01 +/- 130.51
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | -78         |
| time/                    |             |
|    total_timesteps       | 120000      |
| trading/                 |             |
|    episode_pnl_pct       | -0.259      |
|    episode_reward        | -25.6       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 98.8        |
|    trades_count          | 22          |
|    win_rate              | 13.6        |
| train/                   |             |
|    approx_kl             | 0.014225389 |
|    clip_fraction         | 0.0797      |
|    clip_range            | 0.243       |
|    entropy_loss          | -2.11       |
|    explained_variance    | -0.0049     |
|    learning_rate         | 0.0003      |
|    loss                  | 241         |
|    n_updates             | 119         |
|    policy_gradient_loss  | -0.0311     |
|    value_loss            | 1.62e+03    |
------------------------------------------
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -28.7    |
| time/                    |          |
|    fps                   | 366      |
|    iterations            | 8        |
|    time_elapsed          | 357      |
|    total_timesteps       | 131072   |
| trading/                 |          |
|    episode_pnl_pct       | -0.265   |
|    episode_reward        | -11.7    |
|    final_portfolio_value | 9.97e+03 |
|    reward_per_pnl        | 44.2     |
|    trades_count          | 23       |
|    win_rate              | 17.4     |
| train/                   |          |
|    ent_coef              | 0.0407   |
---------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -13.8       |
| time/                    |             |
|    fps                   | 369         |
|    iterations            | 9           |
|    time_elapsed          | 399         |
|    total_timesteps       | 147456      |
| trading/                 |             |
|    episode_pnl_pct       | -1.21       |
|    episode_reward        | -96.9       |
|    final_portfolio_value | 9.88e+03    |
|    reward_per_pnl        | 79.9        |
|    trades_count          | 25          |
|    win_rate              | 28          |
| train/                   |             |
|    approx_kl             | 0.015658844 |
|    clip_fraction         | 0.103       |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0358      |
|    entropy_loss          | -2.11       |
|    explained_variance    | -0.0299     |
|    learning_rate         | 0.0003      |
|    loss                  | 616         |
|    n_updates             | 136         |
|    policy_gradient_loss  | -0.037      |
|    value_loss            | 827         |
------------------------------------------
Eval num_timesteps=160000, episode_reward=12.90 +/- 43.10
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | 12.9        |
| time/                    |             |
|    total_timesteps       | 160000      |
| trading/                 |             |
|    episode_pnl_pct       | -0.177      |
|    episode_reward        | -17.8       |
|    final_portfolio_value | 9.98e+03    |
|    reward_per_pnl        | 101         |
|    trades_count          | 22          |
|    win_rate              | 18.2        |
| train/                   |             |
|    approx_kl             | 0.018250097 |
|    clip_fraction         | 0.116       |
|    clip_range            | 0.243       |
|    entropy_loss          | -2.08       |
|    explained_variance    | -0.0027     |
|    learning_rate         | 0.0003      |
|    loss                  | 217         |
|    n_updates             | 153         |
|    policy_gradient_loss  | -0.0391     |
|    value_loss            | 644         |
------------------------------------------
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -16.3    |
| time/                    |          |
|    fps                   | 368      |
|    iterations            | 10       |
|    time_elapsed          | 444      |
|    total_timesteps       | 163840   |
| trading/                 |          |
|    episode_pnl_pct       | -0.312   |
|    episode_reward        | -33.8    |
|    final_portfolio_value | 9.97e+03 |
|    reward_per_pnl        | 109      |
|    trades_count          | 18       |
|    win_rate              | 11.1     |
| train/                   |          |
|    ent_coef              | 0.0308   |
---------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -17.4       |
| time/                    |             |
|    fps                   | 369         |
|    iterations            | 11          |
|    time_elapsed          | 487         |
|    total_timesteps       | 180224      |
| trading/                 |             |
|    episode_pnl_pct       | -0.247      |
|    episode_reward        | -22.6       |
|    final_portfolio_value | 9.98e+03    |
|    reward_per_pnl        | 91.4        |
|    trades_count          | 22          |
|    win_rate              | 31.8        |
| train/                   |             |
|    approx_kl             | 0.018039308 |
|    clip_fraction         | 0.116       |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0259      |
|    entropy_loss          | -2.08       |
|    explained_variance    | 0.0214      |
|    learning_rate         | 0.0003      |
|    loss                  | 148         |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.0411     |
|    value_loss            | 585         |
------------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -17.5       |
| time/                    |             |
|    fps                   | 373         |
|    iterations            | 12          |
|    time_elapsed          | 526         |
|    total_timesteps       | 196608      |
| trading/                 |             |
|    episode_pnl_pct       | -1.05       |
|    episode_reward        | -75.3       |
|    final_portfolio_value | 9.89e+03    |
|    reward_per_pnl        | 71.5        |
|    trades_count          | 18          |
|    win_rate              | 22.2        |
| train/                   |             |
|    approx_kl             | 0.016439853 |
|    clip_fraction         | 0.105       |
|    clip_range            | 0.243       |
|    ent_coef              | 0.021       |
|    entropy_loss          | -2.07       |
|    explained_variance    | 0.0404      |
|    learning_rate         | 0.0003      |
|    loss                  | 230         |
|    n_updates             | 187         |
|    policy_gradient_loss  | -0.0379     |
|    value_loss            | 1.03e+03    |
------------------------------------------
Eval num_timesteps=200000, episode_reward=-8.58 +/- 46.74
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | -8.58       |
| time/                    |             |
|    total_timesteps       | 200000      |
| trading/                 |             |
|    episode_pnl_pct       | -0.39       |
|    episode_reward        | -11.8       |
|    final_portfolio_value | 9.96e+03    |
|    reward_per_pnl        | 30.2        |
|    trades_count          | 26          |
|    win_rate              | 15.4        |
| train/                   |             |
|    approx_kl             | 0.019164585 |
|    clip_fraction         | 0.133       |
|    clip_range            | 0.243       |
|    entropy_loss          | -2.05       |
|    explained_variance    | 0.0506      |
|    learning_rate         | 0.0003      |
|    loss                  | 308         |
|    n_updates             | 204         |
|    policy_gradient_loss  | -0.0417     |
|    value_loss            | 849         |
------------------------------------------
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -1.07    |
| time/                    |          |
|    fps                   | 366      |
|    iterations            | 13       |
|    time_elapsed          | 580      |
|    total_timesteps       | 212992   |
| trading/                 |          |
|    episode_pnl_pct       | -0.545   |
|    episode_reward        | -42.5    |
|    final_portfolio_value | 9.95e+03 |
|    reward_per_pnl        | 77.9     |
|    trades_count          | 23       |
|    win_rate              | 8.7      |
| train/                   |          |
|    ent_coef              | 0.02     |
---------------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 212,992/200,000  [ 0:09:03 < 0:00:00 , 357 it/s ]
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

✅ Training complete!
Final model saved to: models/rl_v3/ppo_20251031_114621/final_model
Best model saved to: models/rl_v3/ppo_20251031_114621/best_model

Evaluating best model on test set...

================================================================================
EVALUATING TRAINED AGENT
================================================================================
Loading model from: models/rl_v3/ppo_20251031_114621/best_model/best_model.zip
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
Episode 1/10: Reward=+581.40, P&L=+1.90%, Steps=72
Episode 2/10: Reward=+1281.84, P&L=+3.61%, Steps=72
Episode 3/10: Reward=+496.95, P&L=+1.24%, Steps=72
Episode 4/10: Reward=-847.01, P&L=-3.24%, Steps=72
Episode 5/10: Reward=-100.40, P&L=-0.53%, Steps=72
Episode 6/10: Reward=-100.40, P&L=-0.53%, Steps=72
Episode 7/10: Reward=-96.06, P&L=-0.51%, Steps=72
Episode 8/10: Reward=+1291.14, P&L=+3.61%, Steps=72
Episode 9/10: Reward=+8.11, P&L=-0.27%, Steps=72
Episode 10/10: Reward=+405.94, P&L=+0.75%, Steps=72

────────────────────────────────────────────────────────────
EVALUATION SUMMARY
────────────────────────────────────────────────────────────
Episodes: 10
Average Reward: +292.15 ± 628.75
Average P&L: +0.60% ± 1.99%
Average Length: 72.0 steps
Win Rate: 50.0%
────────────────────────────────────────────────────────────

TOP 5 MOST PROFITABLE TRADES:
────────────────────────────────────────────────────────────────────────────────
1. FUSDT | Entry: 2025-10-25 00:22 | P&L: $+230.55 (+3.46%) | Duration: 8.0h
2. FUSDT | Entry: 2025-10-25 00:24 | P&L: $+230.52 (+3.46%) | Duration: 8.0h
3. FUSDT | Entry: 2025-10-25 00:21 | P&L: $+229.90 (+3.46%) | Duration: 8.0h
4. COAIUSDT | Entry: 2025-10-26 18:49 | P&L: $+152.13 (+2.29%) | Duration: 9.0h
5. COAIUSDT | Entry: 2025-10-24 04:07 | P&L: $+136.09 (+2.04%) | Duration: 11.0h
────────────────────────────────────────────────────────────────────────────────

✅ All trades saved to: evaluation_trades_20251031_115635.csv
   Total trades: 228
   Winning trades: 53
   Losing trades: 175
