================================================================================
TRAINING PPO AGENT FOR FUNDING ARBITRAGE
================================================================================
Data: data/rl_train.csv
Total timesteps: 1,000,000
Learning rate: 0.0003
Seed: 42

Creating training environment with 8 parallel workers...
Creating evaluation environment...
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

Initializing PPO agent...
💻 Using CPU (faster than MPS for MLP + small batches)
Using cpu device

Model architecture:
  Policy network: MLP (256 → 256 hidden layers)
  Value network: MLP (256 → 256 hidden layers)
  Total parameters: ~200K per network
  Observation space: 124 dimensions
  Action space: 9 discrete actions
  Entropy: 0.0800 → 0.0200 (annealed via callback)
  Gamma: 0.9600
  GAE Lambda: 0.9888
  Clip range: 0.243
  Reward scale: 1.000
  Hold bonus: 0.000
  Quality entry bonus: 0.500
  Quality entry penalty: -0.500

================================================================================
STARTING TRAINING
================================================================================
Training for 1,000,000 timesteps...
Progress will be logged to: models/rl/ppo_20251031_103918

Logging to models/rl/ppo_20251031_103918/tensorboard/PPO_1
/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x3556581c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x355658190>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 417      |
|    iterations      | 1        |
|    time_elapsed    | 39       |
|    total_timesteps | 16384    |
| train/             |          |
|    ent_coef        | 0.079    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 393         |
|    iterations           | 2           |
|    time_elapsed         | 83          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.012664406 |
|    clip_fraction        | 0.0655      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.078       |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.00108     |
|    learning_rate        | 0.0003      |
|    loss                 | 481         |
|    n_updates            | 17          |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 2.03e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1330.17 +/- 372.84
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 1.33e+03     |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0105994595 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.0524       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+03     |
|    n_updates            | 34           |
|    policy_gradient_loss | -0.02        |
|    value_loss           | 2.48e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 386      |
|    iterations      | 3        |
|    time_elapsed    | 127      |
|    total_timesteps | 49152    |
| train/             |          |
|    ent_coef        | 0.0771   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 204         |
| time/                   |             |
|    fps                  | 406         |
|    iterations           | 4           |
|    time_elapsed         | 161         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.010151882 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0761      |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.0533      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62e+03    |
|    n_updates            | 51          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 3.75e+03    |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=1723.22 +/- 468.16
Episode length: 72.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72       |
|    mean_reward          | 1.72e+03 |
| time/                   |          |
|    total_timesteps      | 80000    |
| train/                  |          |
|    approx_kl            | 0.007228 |
|    clip_fraction        | 0.0252   |
|    clip_range           | 0.243    |
|    entropy_loss         | -2.13    |
|    explained_variance   | 0.142    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.98e+03 |
|    n_updates            | 68       |
|    policy_gradient_loss | -0.015   |
|    value_loss           | 4.81e+03 |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 220      |
| time/              |          |
|    fps             | 415      |
|    iterations      | 5        |
|    time_elapsed    | 196      |
|    total_timesteps | 81920    |
| train/             |          |
|    ent_coef        | 0.0751   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 419        |
|    iterations           | 6          |
|    time_elapsed         | 234        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.00589926 |
|    clip_fraction        | 0.021      |
|    clip_range           | 0.243      |
|    ent_coef             | 0.0741     |
|    entropy_loss         | -2.12      |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51e+03   |
|    n_updates            | 85         |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 5.44e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 344         |
| time/                   |             |
|    fps                  | 429         |
|    iterations           | 7           |
|    time_elapsed         | 267         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.005257516 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0731      |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.08e+03    |
|    n_updates            | 102         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 7.95e+03    |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=2040.05 +/- 448.11
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.04e+03     |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0050418368 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.21e+03     |
|    n_updates            | 119          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 8.53e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 426      |
|    iterations      | 8        |
|    time_elapsed    | 307      |
|    total_timesteps | 131072   |
| train/             |          |
|    ent_coef        | 0.0721   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 513          |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 9            |
|    time_elapsed         | 341          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0049553374 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0712       |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.426        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.14e+03     |
|    n_updates            | 136          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 9e+03        |
------------------------------------------
Eval num_timesteps=160000, episode_reward=1967.08 +/- 416.66
Episode length: 72.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72         |
|    mean_reward          | 1.97e+03   |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.00321178 |
|    clip_fraction        | 0.00735    |
|    clip_range           | 0.243      |
|    entropy_loss         | -2.02      |
|    explained_variance   | 0.444      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.14e+03   |
|    n_updates            | 153        |
|    policy_gradient_loss | -0.00683   |
|    value_loss           | 1.03e+04   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 436      |
|    iterations      | 10       |
|    time_elapsed    | 375      |
|    total_timesteps | 163840   |
| train/             |          |
|    ent_coef        | 0.0702   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 430          |
| time/                   |              |
|    fps                  | 439          |
|    iterations           | 11           |
|    time_elapsed         | 410          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0026087563 |
|    clip_fraction        | 0.00428      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0692       |
|    entropy_loss         | -2.01        |
|    explained_variance   | 0.474        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.24e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00555     |
|    value_loss           | 1.12e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 508          |
| time/                   |              |
|    fps                  | 443          |
|    iterations           | 12           |
|    time_elapsed         | 442          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0029951497 |
|    clip_fraction        | 0.00875      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0682       |
|    entropy_loss         | -2           |
|    explained_variance   | 0.473        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.7e+03      |
|    n_updates            | 187          |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=200000, episode_reward=2277.35 +/- 467.69
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 2.28e+03    |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.002904185 |
|    clip_fraction        | 0.00471     |
|    clip_range           | 0.243       |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.05e+03    |
|    n_updates            | 204         |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 1.18e+04    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 610      |
| time/              |          |
|    fps             | 444      |
|    iterations      | 13       |
|    time_elapsed    | 479      |
|    total_timesteps | 212992   |
| train/             |          |
|    ent_coef        | 0.0672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 618         |
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 14          |
|    time_elapsed         | 512         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.002419531 |
|    clip_fraction        | 0.00463     |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0662      |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.91e+03    |
|    n_updates            | 221         |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 1.38e+04    |
-----------------------------------------
Eval num_timesteps=240000, episode_reward=2367.03 +/- 640.10
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.37e+03     |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0023526787 |
|    clip_fraction        | 0.00217      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.612        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.07e+03     |
|    n_updates            | 238          |
|    policy_gradient_loss | -0.00412     |
|    value_loss           | 1.31e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 445      |
|    iterations      | 15       |
|    time_elapsed    | 551      |
|    total_timesteps | 245760   |
| train/             |          |
|    ent_coef        | 0.0653   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 622          |
| time/                   |              |
|    fps                  | 446          |
|    iterations           | 16           |
|    time_elapsed         | 587          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0017518599 |
|    clip_fraction        | 0.00296      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0643       |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.589        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.88e+03     |
|    n_updates            | 255          |
|    policy_gradient_loss | -0.00384     |
|    value_loss           | 1.21e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 531          |
| time/                   |              |
|    fps                  | 447          |
|    iterations           | 17           |
|    time_elapsed         | 621          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0024248837 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0633       |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.61e+03     |
|    n_updates            | 272          |
|    policy_gradient_loss | -0.00415     |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=280000, episode_reward=2434.98 +/- 150.74
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.43e+03     |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0016637864 |
|    clip_fraction        | 0.00219      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.618        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.56e+03     |
|    n_updates            | 289          |
|    policy_gradient_loss | -0.00321     |
|    value_loss           | 1.09e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 631      |
| time/              |          |
|    fps             | 444      |
|    iterations      | 18       |
|    time_elapsed    | 662      |
|    total_timesteps | 294912   |
| train/             |          |
|    ent_coef        | 0.0623   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 524          |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 19           |
|    time_elapsed         | 699          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0015955139 |
|    clip_fraction        | 0.0027       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0613       |
|    entropy_loss         | -1.89        |
|    explained_variance   | 0.594        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.23e+03     |
|    n_updates            | 306          |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=2228.31 +/- 134.29
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.23e+03     |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0014734145 |
|    clip_fraction        | 0.00194      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.635        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.42e+03     |
|    n_updates            | 323          |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 9.47e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 582      |
| time/              |          |
|    fps             | 443      |
|    iterations      | 20       |
|    time_elapsed    | 738      |
|    total_timesteps | 327680   |
| train/             |          |
|    ent_coef        | 0.0603   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 638          |
| time/                   |              |
|    fps                  | 446          |
|    iterations           | 21           |
|    time_elapsed         | 771          |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0019149506 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0594       |
|    entropy_loss         | -1.88        |
|    explained_variance   | 0.644        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.87e+03     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00329     |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=360000, episode_reward=2545.19 +/- 564.00
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.55e+03     |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0018331367 |
|    clip_fraction        | 0.00506      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.674        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.77e+03     |
|    n_updates            | 357          |
|    policy_gradient_loss | -0.00383     |
|    value_loss           | 1.03e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 621      |
| time/              |          |
|    fps             | 448      |
|    iterations      | 22       |
|    time_elapsed    | 804      |
|    total_timesteps | 360448   |
| train/             |          |
|    ent_coef        | 0.0584   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 646          |
| time/                   |              |
|    fps                  | 448          |
|    iterations           | 23           |
|    time_elapsed         | 840          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0015230041 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0574       |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.53e+03     |
|    n_updates            | 374          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 9.64e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 643         |
| time/                   |             |
|    fps                  | 449         |
|    iterations           | 24          |
|    time_elapsed         | 874         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.001064021 |
|    clip_fraction        | 0.00169     |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0564      |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.29e+03    |
|    n_updates            | 391         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 1.1e+04     |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=2572.11 +/- 323.26
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.57e+03     |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0011516805 |
|    clip_fraction        | 0.00207      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.88        |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.48e+03     |
|    n_updates            | 408          |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 1.13e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 449      |
|    iterations      | 25       |
|    time_elapsed    | 911      |
|    total_timesteps | 409600   |
| train/             |          |
|    ent_coef        | 0.0554   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 684          |
| time/                   |              |
|    fps                  | 450          |
|    iterations           | 26           |
|    time_elapsed         | 946          |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0015091465 |
|    clip_fraction        | 0.00224      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0544       |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.703        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.81e+03     |
|    n_updates            | 425          |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=440000, episode_reward=2006.89 +/- 301.20
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 2.01e+03    |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.002028422 |
|    clip_fraction        | 0.00429     |
|    clip_range           | 0.243       |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.96e+03    |
|    n_updates            | 442         |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 1.13e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 680      |
| time/              |          |
|    fps             | 450      |
|    iterations      | 27       |
|    time_elapsed    | 981      |
|    total_timesteps | 442368   |
| train/             |          |
|    ent_coef        | 0.0535   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 712          |
| time/                   |              |
|    fps                  | 451          |
|    iterations           | 28           |
|    time_elapsed         | 1016         |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0018208132 |
|    clip_fraction        | 0.00294      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0525       |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.714        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.77e+03     |
|    n_updates            | 459          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 1.26e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 660         |
| time/                   |             |
|    fps                  | 450         |
|    iterations           | 29          |
|    time_elapsed         | 1055        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.001820223 |
|    clip_fraction        | 0.00138     |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0515      |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.01e+03    |
|    n_updates            | 476         |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=2291.17 +/- 221.04
Episode length: 72.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 72            |
|    mean_reward          | 2.29e+03      |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 0.00063699554 |
|    clip_fraction        | 0.000478      |
|    clip_range           | 0.243         |
|    entropy_loss         | -1.83         |
|    explained_variance   | 0.735         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.79e+03      |
|    n_updates            | 493           |
|    policy_gradient_loss | -0.0018       |
|    value_loss           | 1.21e+04      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 663      |
| time/              |          |
|    fps             | 446      |
|    iterations      | 30       |
|    time_elapsed    | 1101     |
|    total_timesteps | 491520   |
| train/             |          |
|    ent_coef        | 0.0505   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 834          |
| time/                   |              |
|    fps                  | 444          |
|    iterations           | 31           |
|    time_elapsed         | 1142         |
|    total_timesteps      | 507904       |
| train/                  |              |
|    approx_kl            | 0.0014780517 |
|    clip_fraction        | 0.000772     |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0495       |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.34e+03     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=520000, episode_reward=2825.39 +/- 274.98
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.83e+03     |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0010520777 |
|    clip_fraction        | 0.00159      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.754        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.19e+03     |
|    n_updates            | 527          |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 1.19e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 443      |
|    iterations      | 32       |
|    time_elapsed    | 1182     |
|    total_timesteps | 524288   |
| train/             |          |
|    ent_coef        | 0.0485   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 801          |
| time/                   |              |
|    fps                  | 441          |
|    iterations           | 33           |
|    time_elapsed         | 1225         |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0018507796 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0476       |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.37e+03     |
|    n_updates            | 544          |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 1.01e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 633          |
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 34           |
|    time_elapsed         | 1265         |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0016336596 |
|    clip_fraction        | 0.00188      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0466       |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.793        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.19e+03     |
|    n_updates            | 561          |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 9.42e+03     |
------------------------------------------
Eval num_timesteps=560000, episode_reward=2292.25 +/- 552.43
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.29e+03     |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0009554161 |
|    clip_fraction        | 0.000747     |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.64e+03     |
|    n_updates            | 578          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 1.31e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 436      |
|    iterations      | 35       |
|    time_elapsed    | 1313     |
|    total_timesteps | 573440   |
| train/             |          |
|    ent_coef        | 0.0456   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 794         |
| time/                   |             |
|    fps                  | 433         |
|    iterations           | 36          |
|    time_elapsed         | 1360        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.001788917 |
|    clip_fraction        | 0.00212     |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0446      |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.31e+03    |
|    n_updates            | 595         |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 9.61e+03    |
-----------------------------------------
Eval num_timesteps=600000, episode_reward=1927.21 +/- 177.44
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 1.93e+03     |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0021307464 |
|    clip_fraction        | 0.00152      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.767        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.66e+03     |
|    n_updates            | 612          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 1.11e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 430      |
|    iterations      | 37       |
|    time_elapsed    | 1409     |
|    total_timesteps | 606208   |
| train/             |          |
|    ent_coef        | 0.0436   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 741          |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 38           |
|    time_elapsed         | 1453         |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 0.0014596246 |
|    clip_fraction        | 0.00168      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0426       |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.22e+03     |
|    n_updates            | 629          |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 1.11e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 804          |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 39           |
|    time_elapsed         | 1494         |
|    total_timesteps      | 638976       |
| train/                  |              |
|    approx_kl            | 0.0020280597 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0417       |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.41e+03     |
|    n_updates            | 646          |
|    policy_gradient_loss | -0.00313     |
|    value_loss           | 8.57e+03     |
------------------------------------------
Eval num_timesteps=640000, episode_reward=2391.06 +/- 440.47
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 2.39e+03    |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.002520902 |
|    clip_fraction        | 0.0024      |
|    clip_range           | 0.243       |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.81e+03    |
|    n_updates            | 663         |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 9.45e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 423      |
|    iterations      | 40       |
|    time_elapsed    | 1546     |
|    total_timesteps | 655360   |
| train/             |          |
|    ent_coef        | 0.0407   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 769          |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 41           |
|    time_elapsed         | 1593         |
|    total_timesteps      | 671744       |
| train/                  |              |
|    approx_kl            | 0.0022076387 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0397       |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.818        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.75e+03     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 7.05e+03     |
------------------------------------------
Eval num_timesteps=680000, episode_reward=2354.70 +/- 304.73
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.35e+03     |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0030298731 |
|    clip_fraction        | 0.00497      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.794        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.36e+03     |
|    n_updates            | 697          |
|    policy_gradient_loss | -0.00472     |
|    value_loss           | 9.52e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 952      |
| time/              |          |
|    fps             | 419      |
|    iterations      | 42       |
|    time_elapsed    | 1639     |
|    total_timesteps | 688128   |
| train/             |          |
|    ent_coef        | 0.0387   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 773          |
| time/                   |              |
|    fps                  | 418          |
|    iterations           | 43           |
|    time_elapsed         | 1683         |
|    total_timesteps      | 704512       |
| train/                  |              |
|    approx_kl            | 0.0012280082 |
|    clip_fraction        | 0.00165      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0377       |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.65e+03     |
|    n_updates            | 714          |
|    policy_gradient_loss | -0.00172     |
|    value_loss           | 1.08e+04     |
------------------------------------------
Eval num_timesteps=720000, episode_reward=2189.18 +/- 312.29
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.19e+03     |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0014329491 |
|    clip_fraction        | 0.00135      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.12e+03     |
|    n_updates            | 731          |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 1.03e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 417      |
|    iterations      | 44       |
|    time_elapsed    | 1728     |
|    total_timesteps | 720896   |
| train/             |          |
|    ent_coef        | 0.0367   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 72            |
|    ep_rew_mean          | 741           |
| time/                   |               |
|    fps                  | 415           |
|    iterations           | 45            |
|    time_elapsed         | 1774          |
|    total_timesteps      | 737280        |
| train/                  |               |
|    approx_kl            | 0.00090187823 |
|    clip_fraction        | 0.000582      |
|    clip_range           | 0.243         |
|    ent_coef             | 0.0358        |
|    entropy_loss         | -1.68         |
|    explained_variance   | 0.79          |
|    learning_rate        | 0.0003        |
|    loss                 | 6e+03         |
|    n_updates            | 748           |
|    policy_gradient_loss | -0.00183      |
|    value_loss           | 9.99e+03      |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 917         |
| time/                   |             |
|    fps                  | 415         |
|    iterations           | 46          |
|    time_elapsed         | 1813        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.002989566 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0348      |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.67e+03    |
|    n_updates            | 765         |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 8.19e+03    |
-----------------------------------------
Eval num_timesteps=760000, episode_reward=2440.28 +/- 502.50
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.44e+03     |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0018321415 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.44e+03     |
|    n_updates            | 782          |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 9.66e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 988      |
| time/              |          |
|    fps             | 414      |
|    iterations      | 47       |
|    time_elapsed    | 1857     |
|    total_timesteps | 770048   |
| train/             |          |
|    ent_coef        | 0.0338   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 72            |
|    ep_rew_mean          | 798           |
| time/                   |               |
|    fps                  | 414           |
|    iterations           | 48            |
|    time_elapsed         | 1897          |
|    total_timesteps      | 786432        |
| train/                  |               |
|    approx_kl            | 0.00094597635 |
|    clip_fraction        | 0.000743      |
|    clip_range           | 0.243         |
|    ent_coef             | 0.0328        |
|    entropy_loss         | -1.59         |
|    explained_variance   | 0.848         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.75e+03      |
|    n_updates            | 799           |
|    policy_gradient_loss | -0.00239      |
|    value_loss           | 8.43e+03      |
-------------------------------------------
Eval num_timesteps=800000, episode_reward=2708.07 +/- 247.60
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.71e+03     |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 0.0012303429 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.48e+03     |
|    n_updates            | 816          |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 8.96e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 414      |
|    iterations      | 49       |
|    time_elapsed    | 1937     |
|    total_timesteps | 802816   |
| train/             |          |
|    ent_coef        | 0.0318   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 997          |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 50           |
|    time_elapsed         | 1981         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0034566629 |
|    clip_fraction        | 0.00984      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0308       |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.46e+03     |
|    n_updates            | 833          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 9.25e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 899          |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 51           |
|    time_elapsed         | 2022         |
|    total_timesteps      | 835584       |
| train/                  |              |
|    approx_kl            | 0.0019684792 |
|    clip_fraction        | 0.00276      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0299       |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.0003       |
|    loss                 | 5.06e+03     |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=840000, episode_reward=2848.80 +/- 457.74
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.85e+03     |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0009825181 |
|    clip_fraction        | 0.000467     |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.17e+03     |
|    n_updates            | 867          |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 8.82e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 915      |
| time/              |          |
|    fps             | 411      |
|    iterations      | 52       |
|    time_elapsed    | 2069     |
|    total_timesteps | 851968   |
| train/             |          |
|    ent_coef        | 0.0289   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 954          |
| time/                   |              |
|    fps                  | 410          |
|    iterations           | 53           |
|    time_elapsed         | 2117         |
|    total_timesteps      | 868352       |
| train/                  |              |
|    approx_kl            | 0.0016972974 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0279       |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.826        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.66e+03     |
|    n_updates            | 884          |
|    policy_gradient_loss | -0.00354     |
|    value_loss           | 9.47e+03     |
------------------------------------------
Eval num_timesteps=880000, episode_reward=2736.25 +/- 409.29
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.74e+03     |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0010418701 |
|    clip_fraction        | 0.0014       |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.05e+03     |
|    n_updates            | 901          |
|    policy_gradient_loss | -0.00164     |
|    value_loss           | 9.99e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 408      |
|    iterations      | 54       |
|    time_elapsed    | 2165     |
|    total_timesteps | 884736   |
| train/             |          |
|    ent_coef        | 0.0269   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 902          |
| time/                   |              |
|    fps                  | 406          |
|    iterations           | 55           |
|    time_elapsed         | 2215         |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0013093739 |
|    clip_fraction        | 0.000984     |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0259       |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.28e+03     |
|    n_updates            | 918          |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 8.74e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 754          |
| time/                   |              |
|    fps                  | 405          |
|    iterations           | 56           |
|    time_elapsed         | 2265         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0012804575 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0249       |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.95e+03     |
|    n_updates            | 935          |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 8.91e+03     |
------------------------------------------
Eval num_timesteps=920000, episode_reward=2007.81 +/- 367.51
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.01e+03     |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0012651713 |
|    clip_fraction        | 0.00201      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.02e+03     |
|    n_updates            | 952          |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 7.55e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 402      |
|    iterations      | 57       |
|    time_elapsed    | 2318     |
|    total_timesteps | 933888   |
| train/             |          |
|    ent_coef        | 0.024    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 1.05e+03     |
| time/                   |              |
|    fps                  | 402          |
|    iterations           | 58           |
|    time_elapsed         | 2358         |
|    total_timesteps      | 950272       |
| train/                  |              |
|    approx_kl            | 0.0015775885 |
|    clip_fraction        | 0.00169      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.023        |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.833        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.2e+03      |
|    n_updates            | 969          |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 8.71e+03     |
------------------------------------------
Eval num_timesteps=960000, episode_reward=2178.09 +/- 485.00
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 2.18e+03     |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0010113491 |
|    clip_fraction        | 0.00047      |
|    clip_range           | 0.243        |
|    entropy_loss         | -1.51        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.24e+03     |
|    n_updates            | 986          |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 1.11e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 401      |
|    iterations      | 59       |
|    time_elapsed    | 2406     |
|    total_timesteps | 966656   |
| train/             |          |
|    ent_coef        | 0.022    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 968          |
| time/                   |              |
|    fps                  | 400          |
|    iterations           | 60           |
|    time_elapsed         | 2454         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0012267965 |
|    clip_fraction        | 0.00144      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.021        |
|    entropy_loss         | -1.51        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.05e+03     |
|    n_updates            | 1003         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 8.33e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 911          |
| time/                   |              |
|    fps                  | 400          |
|    iterations           | 61           |
|    time_elapsed         | 2496         |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 0.0014208928 |
|    clip_fraction        | 0.00192      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.02         |
|    entropy_loss         | -1.5         |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.03e+03     |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 8.82e+03     |
------------------------------------------
Eval num_timesteps=1000000, episode_reward=2447.85 +/- 222.23
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 2.45e+03    |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.002259945 |
|    clip_fraction        | 0.00232     |
|    clip_range           | 0.243       |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.35e+03    |
|    n_updates            | 1037        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 9.16e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 919      |
| time/              |          |
|    fps             | 399      |
|    iterations      | 62       |
|    time_elapsed    | 2543     |
|    total_timesteps | 1015808  |
| train/             |          |
|    ent_coef        | 0.02     |
---------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━ 1,015,808/1,000,000  [ 0:41:49 < 0:00:00 , 481 it/s ]
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

✅ Training complete!
Final model saved to: models/rl/ppo_20251031_103918/final_model
Best model saved to: models/rl/ppo_20251031_103918/best_model

Evaluating best model on test set...

================================================================================
EVALUATING TRAINED AGENT
================================================================================
Loading model from: models/rl/ppo_20251031_103918/best_model/best_model.zip
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
Episode 1/10: Reward=+8446.74, P&L=-3.29%, Steps=72
Episode 2/10: Reward=+7132.30, P&L=-3.12%, Steps=72
Episode 3/10: Reward=+8657.62, P&L=-1.07%, Steps=72
Episode 4/10: Reward=+8316.87, P&L=-0.83%, Steps=72
Episode 5/10: Reward=+8608.66, P&L=-0.80%, Steps=72
Episode 6/10: Reward=+8608.66, P&L=-0.80%, Steps=72
Episode 7/10: Reward=+7455.57, P&L=-0.25%, Steps=72
Episode 8/10: Reward=+7650.19, P&L=-3.33%, Steps=72
Episode 9/10: Reward=+7836.05, P&L=-1.12%, Steps=72
Episode 10/10: Reward=+8406.70, P&L=+0.68%, Steps=72

────────────────────────────────────────────────────────────
EVALUATION SUMMARY
────────────────────────────────────────────────────────────
Episodes: 10
Average Reward: +8111.94 ± 520.77
Average P&L: -1.39% ± 1.31%
Average Length: 72.0 steps
Win Rate: 10.0%
────────────────────────────────────────────────────────────

TOP 5 MOST PROFITABLE TRADES:
────────────────────────────────────────────────────────────────────────────────
1. COAIUSDT | Entry: 2025-10-25 13:07 | P&L: $+152.50 (+2.29%) | Duration: 2.0h
2. COAIUSDT | Entry: 2025-10-26 18:49 | P&L: $+92.43 (+1.42%) | Duration: 1.0h
3. COAIUSDT | Entry: 2025-10-26 04:19 | P&L: $+76.02 (+1.16%) | Duration: 1.0h
4. COAIUSDT | Entry: 2025-10-24 19:49 | P&L: $+69.33 (+1.04%) | Duration: 3.0h
5. COAIUSDT | Entry: 2025-10-24 19:46 | P&L: $+68.86 (+1.04%) | Duration: 3.0h
────────────────────────────────────────────────────────────────────────────────
