"""
Position Snapshot Loader

Loads position snapshot data from CSV files generated by HistoricalCollector.
Used for training exit prediction models.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Tuple, List
from datetime import datetime


class SnapshotLoader:
    """
    Load and prepare position snapshot training data from HistoricalCollector CSV exports.
    """

    def __init__(self, data_path: Optional[Path] = None):
        """
        Initialize snapshot loader.

        Args:
            data_path: Path to snapshot CSV file or directory containing CSV files
        """
        self.data_path = Path(data_path) if data_path else None
        self.df = None

    def load_csv(self, csv_path: Path) -> pd.DataFrame:
        """
        Load snapshot data from a single CSV file.

        Args:
            csv_path: Path to snapshot CSV file

        Returns:
            DataFrame with loaded snapshot data
        """
        print(f"Loading position snapshots from {csv_path}...")

        df = pd.read_csv(csv_path)

        print(f"âœ… Loaded {len(df):,} snapshots")
        print(f"   Columns: {len(df.columns)}")
        print(f"   Date range: {df['snapshot_time'].min()} to {df['snapshot_time'].max()}")
        print(f"   Unique executions: {df['execution_id'].nunique():,}")
        print(f"   Avg snapshots per execution: {len(df) / df['execution_id'].nunique():.1f}")

        # Convert date columns to datetime
        date_columns = ['snapshot_time', 'entry_time']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col])

        self.df = df
        return df

    def load_multiple_csvs(self, csv_dir: Path, pattern: str = "*.csv") -> pd.DataFrame:
        """
        Load and concatenate multiple snapshot CSV files.

        Args:
            csv_dir: Directory containing snapshot CSV files
            pattern: File pattern to match (e.g., "*_snapshots.csv")

        Returns:
            Concatenated DataFrame
        """
        csv_files = list(Path(csv_dir).glob(pattern))

        if not csv_files:
            raise ValueError(f"No CSV files found in {csv_dir} matching pattern {pattern}")

        print(f"Found {len(csv_files)} snapshot CSV files")

        dfs = []
        for csv_file in csv_files:
            df = pd.read_csv(csv_file)
            dfs.append(df)
            print(f"  âœ… Loaded {csv_file.name}: {len(df):,} snapshots")

        combined_df = pd.concat(dfs, ignore_index=True)

        print(f"\nâœ… Total: {len(combined_df):,} snapshots")
        print(f"   Unique executions: {combined_df['execution_id'].nunique():,}")

        # Convert date columns
        date_columns = ['snapshot_time', 'entry_time']
        for col in date_columns:
            if col in combined_df.columns:
                combined_df[col] = pd.to_datetime(combined_df[col])

        self.df = combined_df
        return combined_df

    def get_summary_stats(self) -> dict:
        """
        Get summary statistics of the loaded snapshot data.

        Returns:
            Dictionary with summary statistics
        """
        if self.df is None:
            raise ValueError("No data loaded. Call load_csv() or load_multiple_csvs() first.")

        df = self.df

        # Label distribution
        exit_signals = df['should_exit_now'].sum()
        exit_signal_pct = (exit_signals / len(df)) * 100

        # Exit reasons distribution
        exit_reasons = df[df['should_exit_now'] == 1]['exit_reason'].value_counts()

        # Time in position stats
        time_stats = df['time_in_position_hours'].describe()

        # P&L stats
        pnl_stats = df['current_pnl_percent'].describe()

        stats = {
            'total_snapshots': len(df),
            'unique_executions': df['execution_id'].nunique(),
            'avg_snapshots_per_execution': len(df) / df['execution_id'].nunique(),

            # Label distribution
            'exit_signals': int(exit_signals),
            'exit_signal_percentage': exit_signal_pct,
            'hold_signals': len(df) - exit_signals,

            # Exit reasons
            'exit_reasons': exit_reasons.to_dict(),

            # Symbols
            'unique_symbols': df['symbol'].nunique(),
            'top_symbols': df['symbol'].value_counts().head(5).to_dict(),

            # Strategies
            'unique_strategies': df['strategy'].nunique(),
            'strategy_distribution': df['strategy'].value_counts().to_dict(),

            # Time in position
            'avg_time_in_position_hours': time_stats['mean'],
            'median_time_in_position_hours': time_stats['50%'],
            'max_time_in_position_hours': time_stats['max'],

            # P&L
            'avg_pnl_percent': pnl_stats['mean'],
            'median_pnl_percent': pnl_stats['50%'],
            'min_pnl_percent': pnl_stats['min'],
            'max_pnl_percent': pnl_stats['max'],

            # Date range
            'earliest_snapshot': df['snapshot_time'].min(),
            'latest_snapshot': df['snapshot_time'].max(),
        }

        return stats

    def print_summary(self):
        """
        Print a formatted summary of the loaded data.
        """
        stats = self.get_summary_stats()

        print("\n" + "="*60)
        print("POSITION SNAPSHOT DATA SUMMARY")
        print("="*60)

        print(f"\nðŸ“Š Dataset Size:")
        print(f"   Total snapshots: {stats['total_snapshots']:,}")
        print(f"   Unique executions: {stats['unique_executions']:,}")
        print(f"   Avg snapshots/execution: {stats['avg_snapshots_per_execution']:.1f}")

        print(f"\nðŸŽ¯ Label Distribution:")
        print(f"   Exit signals: {stats['exit_signals']:,} ({stats['exit_signal_percentage']:.2f}%)")
        print(f"   Hold signals: {stats['hold_signals']:,} ({100 - stats['exit_signal_percentage']:.2f}%)")

        print(f"\nðŸšª Exit Reasons:")
        for reason, count in stats['exit_reasons'].items():
            print(f"   {reason}: {count:,}")

        print(f"\nðŸ’° Symbols:")
        print(f"   Unique: {stats['unique_symbols']}")
        print("   Top 5:")
        for symbol, count in stats['top_symbols'].items():
            print(f"     {symbol}: {count:,}")

        print(f"\nðŸ“ˆ Strategies:")
        for strategy, count in stats['strategy_distribution'].items():
            print(f"   {strategy}: {count:,}")

        print(f"\nâ±ï¸  Time in Position:")
        print(f"   Average: {stats['avg_time_in_position_hours']:.2f} hours")
        print(f"   Median: {stats['median_time_in_position_hours']:.2f} hours")
        print(f"   Max: {stats['max_time_in_position_hours']:.2f} hours")

        print(f"\nðŸ’µ P&L Statistics:")
        print(f"   Average: {stats['avg_pnl_percent']:.3f}%")
        print(f"   Median: {stats['median_pnl_percent']:.3f}%")
        print(f"   Range: [{stats['min_pnl_percent']:.3f}%, {stats['max_pnl_percent']:.3f}%]")

        print(f"\nðŸ“… Date Range:")
        print(f"   From: {stats['earliest_snapshot']}")
        print(f"   To: {stats['latest_snapshot']}")

        print("="*60 + "\n")

    def get_feature_columns(self) -> Tuple[List[str], List[str], List[str]]:
        """
        Get categorized feature columns.

        Returns:
            Tuple of (static_features, dynamic_features, label_columns)
        """
        if self.df is None:
            raise ValueError("No data loaded. Call load_csv() or load_multiple_csvs() first.")

        all_cols = self.df.columns.tolist()

        # Identification columns (not features)
        id_cols = ['execution_id', 'snapshot_index', 'snapshot_time', 'entry_time']

        # Label columns (what we're predicting)
        label_cols = [
            'should_exit_now',
            'exit_reason',
            'hours_until_optimal_exit',
            'optimal_exit_pnl_percent',
            'potential_pnl_loss'
        ]

        # Static features (entry state - don't change during position)
        static_features = [col for col in all_cols if col.startswith('entry_') or col in [
            'symbol', 'strategy', 'long_exchange', 'short_exchange',
            'ml_predicted_profit_percent', 'ml_predicted_success_probability', 'ml_predicted_duration_hours'
        ]]

        # Dynamic features (current position state)
        dynamic_features = [col for col in all_cols if col.startswith('current_') or col in [
            'time_in_position_hours',
            'peak_pnl_percent',
            'drawdown_from_peak_percent',
            'max_drawdown_percent',
            'pnl_velocity_per_hour',
            'funding_payments_received',
            'funding_earned_usd',
            'funding_earned_percent',
            'funding_rate_differential_change',
            'funding_reversal_magnitude',
            'spread_change_since_entry_percent',
            'volatility_change_ratio',
            'volume_change_ratio',
            'minutes_to_next_funding',
            'position_maturity',
            'hold_efficiency',
            'optimal_time_reached',
            'consecutive_negative_samples',
            'consecutive_positive_samples'
        ]]

        # Remove duplicates and sort
        static_features = sorted(set(static_features) - set(id_cols) - set(label_cols))
        dynamic_features = sorted(set(dynamic_features) - set(id_cols) - set(label_cols))

        return static_features, dynamic_features, label_cols

    def validate_data(self) -> bool:
        """
        Validate the loaded snapshot data for consistency.

        Returns:
            True if data is valid, raises ValueError otherwise
        """
        if self.df is None:
            raise ValueError("No data loaded. Call load_csv() or load_multiple_csvs() first.")

        df = self.df

        # Check for required columns
        required_cols = [
            'execution_id', 'snapshot_index', 'snapshot_time',
            'should_exit_now', 'current_pnl_percent', 'time_in_position_hours'
        ]

        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        # Check for null values in critical columns
        for col in required_cols:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                print(f"âš ï¸  Warning: {null_count} null values in {col}")

        # Check label validity
        invalid_labels = df[~df['should_exit_now'].isin([0, 1])]
        if len(invalid_labels) > 0:
            raise ValueError(f"Invalid values in should_exit_now: {invalid_labels['should_exit_now'].unique()}")

        # Check for negative execution IDs
        if (df['execution_id'] < 0).any():
            raise ValueError("Found negative execution IDs")

        # Check time consistency
        if (df['time_in_position_hours'] < 0).any():
            raise ValueError("Found negative time_in_position_hours")

        print("âœ… Data validation passed")
        return True


# Example usage
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python snapshot_loader.py <path_to_snapshots.csv>")
        sys.exit(1)

    csv_path = Path(sys.argv[1])

    # Load data
    loader = SnapshotLoader()
    df = loader.load_csv(csv_path)

    # Validate
    loader.validate_data()

    # Print summary
    loader.print_summary()

    # Show feature columns
    static_features, dynamic_features, label_cols = loader.get_feature_columns()
    print(f"\nðŸ“‹ Feature Breakdown:")
    print(f"   Static features: {len(static_features)}")
    print(f"   Dynamic features: {len(dynamic_features)}")
    print(f"   Label columns: {len(label_cols)}")
    print(f"   Total: {len(static_features) + len(dynamic_features)} features")
