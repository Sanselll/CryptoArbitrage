================================================================================
TRAINING PPO AGENT FOR FUNDING ARBITRAGE
================================================================================
Data: data/rl_train.csv
Total timesteps: 80,000
Learning rate: 0.0003
Seed: 42

Creating training environment with 8 parallel workers...
Creating evaluation environment...
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

Initializing PPO agent...
üíª Using CPU (faster than MPS for MLP + small batches)
Using cpu device

Model architecture:
  Policy network: MLP (256 ‚Üí 256 hidden layers)
  Value network: MLP (256 ‚Üí 256 hidden layers)
  Total parameters: ~200K per network
  Observation space: 124 dimensions
  Action space: 9 discrete actions
  Entropy: 0.0800 ‚Üí 0.0200 (annealed via callback)
  Gamma: 0.9900
  GAE Lambda: 0.9800
  Clip range: 0.243
  Reward scale: 1.000
  Hold bonus: 0.000
  Quality entry bonus: 0.500
  Quality entry penalty: -0.500

================================================================================
STARTING TRAINING
================================================================================
Training for 80,000 timesteps...
Progress will be logged to: models/rl_v3_80k/ppo_20251031_142535

Logging to models/rl_v3_80k/ppo_20251031_142535/tensorboard/PPO_1
---------------------------------------
| rollout/                 |          |
|    ep_len_mean           | 72       |
|    ep_rew_mean           | -20.7    |
| time/                    |          |
|    fps                   | 342      |
|    iterations            | 1        |
|    time_elapsed          | 47       |
|    total_timesteps       | 16384    |
| trading/                 |          |
|    episode_pnl_pct       | -0.356   |
|    episode_reward        | -27      |
|    final_portfolio_value | 9.96e+03 |
|    reward_per_pnl        | 75.8     |
|    trades_count          | 22       |
|    win_rate              | 18.2     |
| train/                   |          |
|    ent_coef              | 0.0677   |
---------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -29.3       |
| time/                    |             |
|    fps                   | 347         |
|    iterations            | 2           |
|    time_elapsed          | 94          |
|    total_timesteps       | 32768       |
| trading/                 |             |
|    episode_pnl_pct       | -0.4        |
|    episode_reward        | -30         |
|    final_portfolio_value | 9.96e+03    |
|    reward_per_pnl        | 74.9        |
|    trades_count          | 22          |
|    win_rate              | 13.6        |
| train/                   |             |
|    approx_kl             | 0.014321819 |
|    clip_fraction         | 0.0691      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0554      |
|    entropy_loss          | -2.19       |
|    explained_variance    | -0.00142    |
|    learning_rate         | 0.0003      |
|    loss                  | 226         |
|    n_updates             | 17          |
|    policy_gradient_loss  | -0.0229     |
|    value_loss            | 715         |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-41.27 +/- 67.56
New best mean reward!
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | -41.3       |
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -23.2       |
| time/                    |             |
|    fps                   | 346         |
|    iterations            | 3           |
|    time_elapsed          | 141         |
|    total_timesteps       | 49152       |
| trading/                 |             |
|    episode_pnl_pct       | -0.324      |
|    episode_reward        | -23.5       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 72.5        |
|    trades_count          | 21          |
|    win_rate              | 14.3        |
| train/                   |             |
|    approx_kl             | 0.012452068 |
|    clip_fraction         | 0.0546      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0431      |
|    entropy_loss          | -2.18       |
|    explained_variance    | -0.00545    |
|    learning_rate         | 0.0003      |
|    loss                  | 855         |
|    n_updates             | 34          |
|    policy_gradient_loss  | -0.0252     |
|    value_loss            | 935         |
------------------------------------------
------------------------------------------
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -17.9       |
| time/                    |             |
|    fps                   | 362         |
|    iterations            | 4           |
|    time_elapsed          | 180         |
|    total_timesteps       | 65536       |
| trading/                 |             |
|    episode_pnl_pct       | -0.319      |
|    episode_reward        | -39.1       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 123         |
|    trades_count          | 19          |
|    win_rate              | 10.5        |
| train/                   |             |
|    approx_kl             | 0.015585955 |
|    clip_fraction         | 0.0883      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.0308      |
|    entropy_loss          | -2.16       |
|    explained_variance    | -0.0174     |
|    learning_rate         | 0.0003      |
|    loss                  | 208         |
|    n_updates             | 51          |
|    policy_gradient_loss  | -0.0306     |
|    value_loss            | 676         |
------------------------------------------
Eval num_timesteps=80000, episode_reward=7.34 +/- 35.42
New best mean reward!
------------------------------------------
| eval/                    |             |
|    mean_ep_length        | 72          |
|    mean_reward           | 7.34        |
| rollout/                 |             |
|    ep_len_mean           | 72          |
|    ep_rew_mean           | -15.2       |
| time/                    |             |
|    fps                   | 363         |
|    iterations            | 5           |
|    time_elapsed          | 225         |
|    total_timesteps       | 81920       |
| trading/                 |             |
|    episode_pnl_pct       | -0.271      |
|    episode_reward        | -2.37       |
|    final_portfolio_value | 9.97e+03    |
|    reward_per_pnl        | 8.74        |
|    trades_count          | 23          |
|    win_rate              | 26.1        |
| train/                   |             |
|    approx_kl             | 0.014110852 |
|    clip_fraction         | 0.0843      |
|    clip_range            | 0.243       |
|    ent_coef              | 0.02        |
|    entropy_loss          | -2.15       |
|    explained_variance    | 0.013       |
|    learning_rate         | 0.0003      |
|    loss                  | 272         |
|    n_updates             | 68          |
|    policy_gradient_loss  | -0.0262     |
|    value_loss            | 1.71e+03    |
------------------------------------------
 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 81,920/80,000  [ 0:03:40 < 0:00:00 , 438 it/s ]
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

‚úÖ Training complete!
Final model saved to: models/rl_v3_80k/ppo_20251031_142535/final_model
Best model saved to: models/rl_v3_80k/ppo_20251031_142535/best_model

Evaluating best model on test set...

================================================================================
EVALUATING TRAINED AGENT
================================================================================
Loading model from: models/rl_v3_80k/ppo_20251031_142535/best_model/best_model.zip
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
‚úÖ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
Episode 1/10: Reward=+390.32, P&L=+0.96%, Steps=72
Episode 2/10: Reward=+168.08, P&L=+0.13%, Steps=72
Episode 3/10: Reward=-7.35, P&L=-0.40%, Steps=72
Episode 4/10: Reward=-256.53, P&L=-1.27%, Steps=72
Episode 5/10: Reward=+299.73, P&L=+0.65%, Steps=72
Episode 6/10: Reward=+299.73, P&L=+0.65%, Steps=72
Episode 7/10: Reward=+396.26, P&L=+0.95%, Steps=72
Episode 8/10: Reward=+132.08, P&L=+0.20%, Steps=72
Episode 9/10: Reward=+25.23, P&L=-0.33%, Steps=72
Episode 10/10: Reward=-51.44, P&L=-0.83%, Steps=72

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EVALUATION SUMMARY
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Episodes: 10
Average Reward: +139.61 ¬± 202.23
Average P&L: +0.07% ¬± 0.72%
Average Length: 72.0 steps
Win Rate: 60.0%
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

TOP 5 MOST PROFITABLE TRADES:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. COAIUSDT | Entry: 2025-10-26 21:24 | P&L: $+119.62 (+1.80%) | Duration: 5.0h
2. FUSDT | Entry: 2025-10-25 00:21 | P&L: $+95.02 (+1.43%) | Duration: 3.0h
3. COAIUSDT | Entry: 2025-10-26 21:22 | P&L: $+59.88 (+1.80%) | Duration: 5.0h
4. FUSDT | Entry: 2025-10-26 14:29 | P&L: $+56.96 (+0.86%) | Duration: 5.0h
5. FUSDT | Entry: 2025-10-24 06:48 | P&L: $+39.59 (+0.59%) | Duration: 13.0h
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚úÖ All trades saved to: evaluation_trades_20251031_142954.csv
   Total trades: 230
   Winning trades: 59
   Losing trades: 171
