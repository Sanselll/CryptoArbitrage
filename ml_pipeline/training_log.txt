================================================================================
TRAINING PPO AGENT FOR FUNDING ARBITRAGE
================================================================================
Data: data/rl_train.csv
Total timesteps: 1,000,000
Learning rate: 0.0003
Seed: 42

Creating training environment with 8 parallel workers...
Creating evaluation environment...
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

Initializing PPO agent...
💻 Using CPU (faster than MPS for MLP + small batches)
Using cpu device

Model architecture:
  Policy network: MLP (256 → 256 hidden layers)
  Value network: MLP (256 → 256 hidden layers)
  Total parameters: ~200K per network
  Observation space: 124 dimensions
  Action space: 9 discrete actions
  Entropy: 0.0800 → 0.0200 (annealed via callback)
  Gamma: 0.9600
  GAE Lambda: 0.9888
  Clip range: 0.243
  Reward scale: 3.000
  Hold bonus: 0.000
  Quality entry bonus: 0.500
  Quality entry penalty: -0.500

================================================================================
STARTING TRAINING
================================================================================
Training for 1,000,000 timesteps...
Progress will be logged to: models/rl/ppo_20251031_092055

Logging to models/rl/ppo_20251031_092055/tensorboard/PPO_1
/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x34dc8c490> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x34dc8c460>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -26.3    |
| time/              |          |
|    fps             | 378      |
|    iterations      | 1        |
|    time_elapsed    | 43       |
|    total_timesteps | 16384    |
| train/             |          |
|    ent_coef        | 0.079    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | -43.9       |
| time/                   |             |
|    fps                  | 364         |
|    iterations           | 2           |
|    time_elapsed         | 89          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.008462874 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.078       |
|    entropy_loss         | -2.19       |
|    explained_variance   | -0.000146   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59e+03    |
|    n_updates            | 17          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 6.39e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-129.10 +/- 627.02
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0056864345 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.19        |
|    explained_variance   | -0.0128      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 34           |
|    policy_gradient_loss | -0.0134      |
|    value_loss           | 7.26e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -26      |
| time/              |          |
|    fps             | 336      |
|    iterations      | 3        |
|    time_elapsed    | 145      |
|    total_timesteps | 49152    |
| train/             |          |
|    ent_coef        | 0.0771   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | -47.2       |
| time/                   |             |
|    fps                  | 326         |
|    iterations           | 4           |
|    time_elapsed         | 200         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.008576604 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0761      |
|    entropy_loss         | -2.18       |
|    explained_variance   | -0.0263     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.03e+03    |
|    n_updates            | 51          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 4.89e+03    |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=-335.31 +/- 242.31
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -335        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.005444946 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.00533     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01e+03    |
|    n_updates            | 68          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.82e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -46.9    |
| time/              |          |
|    fps             | 320      |
|    iterations      | 5        |
|    time_elapsed    | 255      |
|    total_timesteps | 81920    |
| train/             |          |
|    ent_coef        | 0.0751   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -36.2        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 6            |
|    time_elapsed         | 311          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0056052655 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0741       |
|    entropy_loss         | -2.17        |
|    explained_variance   | -0.00875     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.72e+03     |
|    n_updates            | 85           |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 1.08e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | -69.3       |
| time/                   |             |
|    fps                  | 311         |
|    iterations           | 7           |
|    time_elapsed         | 368         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.005619503 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0731      |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.0213      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.93e+03    |
|    n_updates            | 102         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 6.23e+03    |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=-182.69 +/- 621.91
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -183         |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0064667445 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.16        |
|    explained_variance   | -0.0377      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.76e+03     |
|    n_updates            | 119          |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 7.52e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -25.5    |
| time/              |          |
|    fps             | 306      |
|    iterations      | 8        |
|    time_elapsed    | 427      |
|    total_timesteps | 131072   |
| train/             |          |
|    ent_coef        | 0.0721   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | -26.8       |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 9           |
|    time_elapsed         | 482         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.005222882 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0712      |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.0282      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95e+03    |
|    n_updates            | 136         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 8.78e+03    |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=149.23 +/- 370.83
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.006450614 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.16       |
|    explained_variance   | -0.0145     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39e+03    |
|    n_updates            | 153         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 7.9e+03     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -6.95    |
| time/              |          |
|    fps             | 305      |
|    iterations      | 10       |
|    time_elapsed    | 537      |
|    total_timesteps | 163840   |
| train/             |          |
|    ent_coef        | 0.0702   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -38.8        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 11           |
|    time_elapsed         | 595          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0053510047 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0692       |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.0128       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.24e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 6.84e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 12           |
|    time_elapsed         | 649          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0061717676 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0682       |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.0215       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.31e+03     |
|    n_updates            | 187          |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 5.24e+03     |
------------------------------------------
Eval num_timesteps=200000, episode_reward=-171.75 +/- 701.69
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -172        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.006293524 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.0198      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.55e+03    |
|    n_updates            | 204         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 6.67e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -69.1    |
| time/              |          |
|    fps             | 302      |
|    iterations      | 13       |
|    time_elapsed    | 705      |
|    total_timesteps | 212992   |
| train/             |          |
|    ent_coef        | 0.0672   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -25.4        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 14           |
|    time_elapsed         | 757          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0053827697 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0662       |
|    entropy_loss         | -2.15        |
|    explained_variance   | 0.00391      |
|    learning_rate        | 0.0003       |
|    loss                 | 2.23e+03     |
|    n_updates            | 221          |
|    policy_gradient_loss | -0.0134      |
|    value_loss           | 8.09e+03     |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-12.06 +/- 274.03
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -12.1        |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0069864066 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.15        |
|    explained_variance   | -0.0051      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+03     |
|    n_updates            | 238          |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 7.06e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -2.75    |
| time/              |          |
|    fps             | 301      |
|    iterations      | 15       |
|    time_elapsed    | 814      |
|    total_timesteps | 245760   |
| train/             |          |
|    ent_coef        | 0.0653   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -49.4        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 16           |
|    time_elapsed         | 866          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0076969713 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0643       |
|    entropy_loss         | -2.15        |
|    explained_variance   | -0.0152      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.15e+04     |
|    n_updates            | 255          |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 6.86e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | -46.6       |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 17          |
|    time_elapsed         | 919         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.006184856 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0633      |
|    entropy_loss         | -2.14       |
|    explained_variance   | -0.0172     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.84e+03    |
|    n_updates            | 272         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 8.68e+03    |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=13.20 +/- 239.80
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | 13.2         |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0071983435 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.14        |
|    explained_variance   | -0.00348     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.91e+04     |
|    n_updates            | 289          |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 1e+04        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -43.5    |
| time/              |          |
|    fps             | 301      |
|    iterations      | 18       |
|    time_elapsed    | 977      |
|    total_timesteps | 294912   |
| train/             |          |
|    ent_coef        | 0.0623   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -9.49        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 19           |
|    time_elapsed         | 1027         |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0066277776 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0613       |
|    entropy_loss         | -2.13        |
|    explained_variance   | -0.00234     |
|    learning_rate        | 0.0003       |
|    loss                 | 988          |
|    n_updates            | 306          |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 8.15e+03     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-29.07 +/- 121.79
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -29.1       |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.006014645 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.0254      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.95e+03    |
|    n_updates            | 323         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 2.07e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 48.1     |
| time/              |          |
|    fps             | 301      |
|    iterations      | 20       |
|    time_elapsed    | 1085     |
|    total_timesteps | 327680   |
| train/             |          |
|    ent_coef        | 0.0603   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 3.98         |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 21           |
|    time_elapsed         | 1141         |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0036064885 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0594       |
|    entropy_loss         | -2.13        |
|    explained_variance   | 0.0485       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.65e+03     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00774     |
|    value_loss           | 2.62e+04     |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-10.26 +/- 581.22
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -10.3        |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0052376045 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.12        |
|    explained_variance   | -0.15        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.14e+03     |
|    n_updates            | 357          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 5.35e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -14.6    |
| time/              |          |
|    fps             | 301      |
|    iterations      | 22       |
|    time_elapsed    | 1194     |
|    total_timesteps | 360448   |
| train/             |          |
|    ent_coef        | 0.0584   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 2.46         |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 23           |
|    time_elapsed         | 1256         |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0072076274 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0574       |
|    entropy_loss         | -2.13        |
|    explained_variance   | -0.00318     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.84e+03     |
|    n_updates            | 374          |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 4.54e+03     |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | -33.7      |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 24         |
|    time_elapsed         | 1312       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.00629842 |
|    clip_fraction        | 0.0237     |
|    clip_range           | 0.243      |
|    ent_coef             | 0.0564     |
|    entropy_loss         | -2.13      |
|    explained_variance   | -0.0436    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.85e+03   |
|    n_updates            | 391        |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 5.61e+03   |
----------------------------------------
Eval num_timesteps=400000, episode_reward=-150.97 +/- 279.46
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0053889025 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.12        |
|    explained_variance   | 0.0285       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+03     |
|    n_updates            | 408          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 6.78e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 25       |
|    time_elapsed    | 1371     |
|    total_timesteps | 409600   |
| train/             |          |
|    ent_coef        | 0.0554   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 1.92         |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 26           |
|    time_elapsed         | 1423         |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 0.0055456734 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0544       |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.0524       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.02e+03     |
|    n_updates            | 425          |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 8.28e+03     |
------------------------------------------
Eval num_timesteps=440000, episode_reward=15.66 +/- 161.90
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 15.7        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.006913757 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.00797     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.21e+03    |
|    n_updates            | 442         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 5.7e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -26.7    |
| time/              |          |
|    fps             | 299      |
|    iterations      | 27       |
|    time_elapsed    | 1474     |
|    total_timesteps | 442368   |
| train/             |          |
|    ent_coef        | 0.0535   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 4.7         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 28          |
|    time_elapsed         | 1528        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.005678395 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0525      |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.1e+03     |
|    n_updates            | 459         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 6.65e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -18          |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 29           |
|    time_elapsed         | 1579         |
|    total_timesteps      | 475136       |
| train/                  |              |
|    approx_kl            | 0.0041989004 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0515       |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.0452       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.94e+03     |
|    n_updates            | 476          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-416.89 +/- 477.52
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.006938104 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.1        |
|    explained_variance   | -0.0936     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.66e+03    |
|    n_updates            | 493         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 5.48e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 300      |
|    iterations      | 30       |
|    time_elapsed    | 1636     |
|    total_timesteps | 491520   |
| train/             |          |
|    ent_coef        | 0.0505   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -5.15        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 31           |
|    time_elapsed         | 1687         |
|    total_timesteps      | 507904       |
| train/                  |              |
|    approx_kl            | 0.0074883597 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0495       |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.0304       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.83e+03     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.0171      |
|    value_loss           | 4.5e+03      |
------------------------------------------
Eval num_timesteps=520000, episode_reward=-308.71 +/- 383.25
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -309        |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.006999432 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.09       |
|    explained_variance   | -0.0516     |
|    learning_rate        | 0.0003      |
|    loss                 | 3.12e+03    |
|    n_updates            | 527         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.09e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 65.2     |
| time/              |          |
|    fps             | 300      |
|    iterations      | 32       |
|    time_elapsed    | 1743     |
|    total_timesteps | 524288   |
| train/             |          |
|    ent_coef        | 0.0485   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 93.2         |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 33           |
|    time_elapsed         | 1799         |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0060174065 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0476       |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.0229       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.15e+04     |
|    n_updates            | 544          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 1.12e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | -5.17        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 34           |
|    time_elapsed         | 1852         |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0060924436 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0466       |
|    entropy_loss         | -2.08        |
|    explained_variance   | 0.0908       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.06e+03     |
|    n_updates            | 561          |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 6.72e+03     |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-84.27 +/- 279.86
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -84.3        |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0070027877 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.08        |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0003       |
|    loss                 | 2.22e+03     |
|    n_updates            | 578          |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 5.94e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 35       |
|    time_elapsed    | 1917     |
|    total_timesteps | 573440   |
| train/             |          |
|    ent_coef        | 0.0456   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 37.9        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 36          |
|    time_elapsed         | 1975        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.003478527 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0446      |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.09e+03    |
|    n_updates            | 595         |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 1.59e+04    |
-----------------------------------------
Eval num_timesteps=600000, episode_reward=-128.50 +/- 188.47
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -128         |
| time/                   |              |
|    total_timesteps      | 600000       |
| train/                  |              |
|    approx_kl            | 0.0066065914 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.14e+03     |
|    n_updates            | 612          |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 7.48e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 37       |
|    time_elapsed    | 2033     |
|    total_timesteps | 606208   |
| train/             |          |
|    ent_coef        | 0.0436   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 22.3        |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 38          |
|    time_elapsed         | 2094        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.004982862 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0426      |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22e+03    |
|    n_updates            | 629         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 6.32e+03    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 57.4       |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 39         |
|    time_elapsed         | 2144       |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.00566837 |
|    clip_fraction        | 0.0206     |
|    clip_range           | 0.243      |
|    ent_coef             | 0.0417     |
|    entropy_loss         | -2.08      |
|    explained_variance   | 0.148      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.3e+03    |
|    n_updates            | 646        |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 4.33e+03   |
----------------------------------------
Eval num_timesteps=640000, episode_reward=-208.95 +/- 535.56
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -209         |
| time/                   |              |
|    total_timesteps      | 640000       |
| train/                  |              |
|    approx_kl            | 0.0044477643 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.0816       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.74e+03     |
|    n_updates            | 663          |
|    policy_gradient_loss | -0.00989     |
|    value_loss           | 1.46e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 40       |
|    time_elapsed    | 2201     |
|    total_timesteps | 655360   |
| train/             |          |
|    ent_coef        | 0.0407   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 33.7        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 41          |
|    time_elapsed         | 2249        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.005048103 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0397      |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64e+03    |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 7.29e+03    |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=-98.60 +/- 335.78
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -98.6        |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0052793752 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.08e+04     |
|    n_updates            | 697          |
|    policy_gradient_loss | -0.00799     |
|    value_loss           | 2.17e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -3.8     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 42       |
|    time_elapsed    | 2306     |
|    total_timesteps | 688128   |
| train/             |          |
|    ent_coef        | 0.0387   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 33.2         |
| time/                   |              |
|    fps                  | 298          |
|    iterations           | 43           |
|    time_elapsed         | 2363         |
|    total_timesteps      | 704512       |
| train/                  |              |
|    approx_kl            | 0.0037812036 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0377       |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.65e+03     |
|    n_updates            | 714          |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 8.96e+03     |
------------------------------------------
Eval num_timesteps=720000, episode_reward=-235.94 +/- 450.02
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -236         |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0060497946 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.0898       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.58e+03     |
|    n_updates            | 731          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 1.27e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 37.6     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 44       |
|    time_elapsed    | 2417     |
|    total_timesteps | 720896   |
| train/             |          |
|    ent_coef        | 0.0367   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 0.023        |
| time/                   |              |
|    fps                  | 298          |
|    iterations           | 45           |
|    time_elapsed         | 2471         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0032758517 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0358       |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.112        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.5e+03      |
|    n_updates            | 748          |
|    policy_gradient_loss | -0.0076      |
|    value_loss           | 1.91e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 51          |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 46          |
|    time_elapsed         | 2522        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.003191865 |
|    clip_fraction        | 0.0099      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0348      |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43e+04    |
|    n_updates            | 765         |
|    policy_gradient_loss | -0.0063     |
|    value_loss           | 2.02e+04    |
-----------------------------------------
Eval num_timesteps=760000, episode_reward=-235.01 +/- 861.77
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -235         |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0043415665 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.117        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.58e+03     |
|    n_updates            | 782          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 8.35e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 62.2     |
| time/              |          |
|    fps             | 299      |
|    iterations      | 47       |
|    time_elapsed    | 2575     |
|    total_timesteps | 770048   |
| train/             |          |
|    ent_coef        | 0.0338   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 16.8        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 48          |
|    time_elapsed         | 2623        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.003929891 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0328      |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.21        |
|    learning_rate        | 0.0003      |
|    loss                 | 3.74e+03    |
|    n_updates            | 799         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 8.37e+03    |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=-7.60 +/- 676.81
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -7.6         |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 0.0036201798 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.26e+03     |
|    n_updates            | 816          |
|    policy_gradient_loss | -0.00868     |
|    value_loss           | 7.95e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 36.6     |
| time/              |          |
|    fps             | 300      |
|    iterations      | 49       |
|    time_elapsed    | 2674     |
|    total_timesteps | 802816   |
| train/             |          |
|    ent_coef        | 0.0318   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 77.8       |
| time/                   |            |
|    fps                  | 300        |
|    iterations           | 50         |
|    time_elapsed         | 2726       |
|    total_timesteps      | 819200     |
| train/                  |            |
|    approx_kl            | 0.00342282 |
|    clip_fraction        | 0.0117     |
|    clip_range           | 0.243      |
|    ent_coef             | 0.0308     |
|    entropy_loss         | -2.05      |
|    explained_variance   | 0.296      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.6e+03    |
|    n_updates            | 833        |
|    policy_gradient_loss | -0.00967   |
|    value_loss           | 1.09e+04   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 77.9       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 51         |
|    time_elapsed         | 2775       |
|    total_timesteps      | 835584     |
| train/                  |            |
|    approx_kl            | 0.00410057 |
|    clip_fraction        | 0.0158     |
|    clip_range           | 0.243      |
|    ent_coef             | 0.0299     |
|    entropy_loss         | -2.04      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.41e+03   |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.00941   |
|    value_loss           | 7.39e+03   |
----------------------------------------
Eval num_timesteps=840000, episode_reward=415.57 +/- 373.85
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.002931384 |
|    clip_fraction        | 0.00731     |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.44e+03    |
|    n_updates            | 867         |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 1.57e+04    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 301      |
|    iterations      | 52       |
|    time_elapsed    | 2827     |
|    total_timesteps | 851968   |
| train/             |          |
|    ent_coef        | 0.0289   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 74.1         |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 53           |
|    time_elapsed         | 2875         |
|    total_timesteps      | 868352       |
| train/                  |              |
|    approx_kl            | 0.0037155617 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0279       |
|    entropy_loss         | -2.03        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.95e+03     |
|    n_updates            | 884          |
|    policy_gradient_loss | -0.00987     |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=880000, episode_reward=-314.19 +/- 497.64
Episode length: 72.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | -314        |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.005006388 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.243       |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.88e+03    |
|    n_updates            | 901         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.02e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 57.4     |
| time/              |          |
|    fps             | 302      |
|    iterations      | 54       |
|    time_elapsed    | 2924     |
|    total_timesteps | 884736   |
| train/             |          |
|    ent_coef        | 0.0269   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 91.8        |
| time/                   |             |
|    fps                  | 303         |
|    iterations           | 55          |
|    time_elapsed         | 2973        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.004528224 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.243       |
|    ent_coef             | 0.0259      |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.98e+03    |
|    n_updates            | 918         |
|    policy_gradient_loss | -0.00926    |
|    value_loss           | 1.16e+04    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 27.2         |
| time/                   |              |
|    fps                  | 303          |
|    iterations           | 56           |
|    time_elapsed         | 3019         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0031523076 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.0249       |
|    entropy_loss         | -2.02        |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.34e+03     |
|    n_updates            | 935          |
|    policy_gradient_loss | -0.00816     |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=920000, episode_reward=-169.41 +/- 429.79
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0054342505 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.12e+03     |
|    n_updates            | 952          |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 6.49e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 42.3     |
| time/              |          |
|    fps             | 304      |
|    iterations      | 57       |
|    time_elapsed    | 3071     |
|    total_timesteps | 933888   |
| train/             |          |
|    ent_coef        | 0.024    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 209          |
| time/                   |              |
|    fps                  | 304          |
|    iterations           | 58           |
|    time_elapsed         | 3119         |
|    total_timesteps      | 950272       |
| train/                  |              |
|    approx_kl            | 0.0040326146 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.243        |
|    ent_coef             | 0.023        |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.0003       |
|    loss                 | 2.52e+03     |
|    n_updates            | 969          |
|    policy_gradient_loss | -0.00691     |
|    value_loss           | 2.79e+04     |
------------------------------------------
Eval num_timesteps=960000, episode_reward=-212.95 +/- 445.58
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -213         |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0021625739 |
|    clip_fraction        | 0.00567      |
|    clip_range           | 0.243        |
|    entropy_loss         | -2.01        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.0003       |
|    loss                 | 6.68e+03     |
|    n_updates            | 986          |
|    policy_gradient_loss | -0.00525     |
|    value_loss           | 3.02e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 304      |
|    iterations      | 59       |
|    time_elapsed    | 3169     |
|    total_timesteps | 966656   |
| train/             |          |
|    ent_coef        | 0.022    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 72           |
|    ep_rew_mean          | 53.8         |
| time/                   |              |
|    fps                  | 305          |
|    iterations           | 60           |
|    time_elapsed         | 3217         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0031423853 |
|    clip_fraction        | 0.00792      |
|    clip_range           | 0.243        |
|    ent_coef             | 0.021        |
|    entropy_loss         | -2.02        |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.15e+03     |
|    n_updates            | 1003         |
|    policy_gradient_loss | -0.0063      |
|    value_loss           | 1.97e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 72          |
|    ep_rew_mean          | 93.4        |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 61          |
|    time_elapsed         | 3268        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.003199649 |
|    clip_fraction        | 0.0096      |
|    clip_range           | 0.243       |
|    ent_coef             | 0.02        |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49e+04    |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 2.34e+04    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-141.95 +/- 215.51
Episode length: 72.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72           |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0030985032 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.243        |
|    entropy_loss         | -2           |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.67e+03     |
|    n_updates            | 1037         |
|    policy_gradient_loss | -0.00848     |
|    value_loss           | 1.06e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 51.2     |
| time/              |          |
|    fps             | 306      |
|    iterations      | 62       |
|    time_elapsed    | 3319     |
|    total_timesteps | 1015808  |
| train/             |          |
|    ent_coef        | 0.02     |
---------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━ 1,015,808/1,000,000  [ 0:54:39 < 0:00:00 , 438 it/s ]
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 154,786 opportunities from 2025-09-01 01:00:00+00:00 to 2025-10-22 00:40:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions

✅ Training complete!
Final model saved to: models/rl/ppo_20251031_092055/final_model
Best model saved to: models/rl/ppo_20251031_092055/best_model

Evaluating best model on test set...

================================================================================
EVALUATING TRAINED AGENT
================================================================================
Loading model from: models/rl/ppo_20251031_092055/best_model/best_model.zip
   Price history loaded from: data/price_history
   Feature scaler loaded from: models/rl/feature_scaler.pkl
   Features will be standardized (mean=0, std=1)
✅ Environment initialized
   Data: 38,697 opportunities from 2025-10-22 00:40:00+00:00 to 2025-10-28 23:55:00+00:00
   Episode length: 72h (3 days)
   Action space: 9 actions
   Observation space: 124 dimensions
Episode 1/10: Reward=+266.23, P&L=+0.28%, Steps=72
Episode 2/10: Reward=+890.67, P&L=+1.84%, Steps=72
Episode 3/10: Reward=+235.42, P&L=+0.06%, Steps=72
Episode 4/10: Reward=-646.98, P&L=-2.92%, Steps=72
Episode 5/10: Reward=+386.94, P&L=+0.23%, Steps=72
Episode 6/10: Reward=+386.94, P&L=+0.23%, Steps=72
Episode 7/10: Reward=-301.01, P&L=-1.74%, Steps=72
Episode 8/10: Reward=+977.37, P&L=+2.11%, Steps=72
Episode 9/10: Reward=-47.97, P&L=-1.35%, Steps=72
Episode 10/10: Reward=+407.49, P&L=+0.71%, Steps=72

────────────────────────────────────────────────────────────
EVALUATION SUMMARY
────────────────────────────────────────────────────────────
Episodes: 10
Average Reward: +255.51 ± 468.93
Average P&L: -0.05% ± 1.48%
Average Length: 72.0 steps
Win Rate: 70.0%
────────────────────────────────────────────────────────────

TOP 5 MOST PROFITABLE TRADES:
────────────────────────────────────────────────────────────────────────────────
1. COAIUSDT | Entry: 2025-10-26 21:22 | P&L: $+148.65 (+2.23%) | Duration: 2.0h
2. COAIUSDT | Entry: 2025-10-26 21:24 | P&L: $+148.31 (+2.23%) | Duration: 2.0h
3. COAIUSDT | Entry: 2025-10-26 18:49 | P&L: $+55.13 (+1.68%) | Duration: 7.0h
4. COAIUSDT | Entry: 2025-10-25 12:07 | P&L: $+38.81 (+1.16%) | Duration: 5.0h
5. COAIUSDT | Entry: 2025-10-25 22:07 | P&L: $+35.94 (+1.07%) | Duration: 8.0h
────────────────────────────────────────────────────────────────────────────────
